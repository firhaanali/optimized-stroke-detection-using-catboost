# -*- coding: utf-8 -*-
"""Optimized_Stroke_Detection_using_Catboost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lVHo748yhpok8YjRTE9_SVOR7dfbDVOg
"""

!pip install optuna
!pip install catboost

import pandas as pd
import numpy as np
import os
import warnings
warnings.filterwarnings('ignore')

# Scikit-learn
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, balanced_accuracy_score, recall_score, confusion_matrix
from sklearn.impute import KNNImputer

# CatBoost dan SMOTE
from catboost import CatBoostClassifier
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN

# Optuna
import optuna

# Set random seed untuk reproducibility
np.random.seed(42)
import random
random.seed(42)

# Set deterministic behavior untuk CatBoost
os.environ['PYTHONHASHSEED'] = '42'

# === LOAD DATA ===
import kagglehub
path = kagglehub.dataset_download("fedesoriano/stroke-prediction-dataset")
csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
df = pd.read_csv(os.path.join(path, csv_files[0]))

# === PREPROCESSING ===
# Handle missing BMI values
df['temp_age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 70, 100], labels=['Young', 'Middle', 'Senior', 'Elderly'])
for age_group in df['temp_age_group'].unique():
    for gender in df['gender'].unique():
        mask = (df['temp_age_group'] == age_group) & (df['gender'] == gender)
        if mask.sum() > 0:
            median_bmi = df.loc[mask, 'bmi'].median()
            if not pd.isna(median_bmi):
                df.loc[mask & df['bmi'].isnull(), 'bmi'] = median_bmi

# Fill remaining missing values
df['bmi'].fillna(df['bmi'].median(), inplace=True)
df = df.drop('temp_age_group', axis=1)

# Remove impossible values
impossible_bmi_mask = (df['bmi'] <= 10) | (df['bmi'] >= 80)
df = df[~impossible_bmi_mask].reset_index(drop=True)

# Encode categorical variables
X = df.drop(columns=['stroke', 'id'])
y = df['stroke']

categorical_columns = X.select_dtypes(include=['object']).columns.tolist()
for col in categorical_columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))

# Final imputation
numerical_cols = X.select_dtypes(include=[np.number]).columns
imputer = KNNImputer(n_neighbors=5)
X[numerical_cols] = imputer.fit_transform(X[numerical_cols])

# === SPLIT DATA ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# === CALCULATE CLASS WEIGHT ===
class_counts = y_train.value_counts()
scale_pos_weight = class_counts[0] / class_counts[1]
print(f"Class distribution in training: {class_counts.to_dict()}")
print(f"Scale pos weight: {scale_pos_weight:.2f}")

# === CALCULATE CLASS WEIGHT ===
class_counts = y_train.value_counts()
scale_pos_weight = class_counts[0] / class_counts[1]
print(f"Class distribution in training: {class_counts.to_dict()}")
print(f"Scale pos weight: {scale_pos_weight:.2f}")

# === COMPARE DIFFERENT SAMPLING STRATEGIES ===
print("\nComparing sampling strategies...")
sampling_strategies = {
    'SMOTE': SMOTE(random_state=42, k_neighbors=3),
    'BorderlineSMOTE': BorderlineSMOTE(random_state=42, k_neighbors=3),
    'ADASYN': ADASYN(random_state=42, n_neighbors=3)
}

best_sampler = None
best_score = 0
sampling_results = {}

# Fixed cross-validation untuk reproducibility
from sklearn.model_selection import StratifiedKFold
cv_splitter = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

for name, sampler in sampling_strategies.items():
    try:
        X_temp, y_temp = sampler.fit_resample(X_train, y_train)
        temp_model = CatBoostClassifier(random_state=42, verbose=False, scale_pos_weight=scale_pos_weight, thread_count=1)
        cv_scores = cross_val_score(temp_model, X_temp, y_temp, cv=cv_splitter, scoring='balanced_accuracy')
        avg_score = cv_scores.mean()
        sampling_results[name] = avg_score
        print(f"{name}: {avg_score:.4f} (Â±{cv_scores.std():.4f})")

        if avg_score > best_score:
            best_score = avg_score
            best_sampler = sampler
    except Exception as e:
        print(f"{name}: Failed - {str(e)}")
        sampling_results[name] = 0

print(f"\nBest sampling strategy: {type(best_sampler).__name__}")

# === APPLY BEST SAMPLING STRATEGY ===
X_train_balanced, y_train_balanced = best_sampler.fit_resample(X_train, y_train)

# === OPTUNA HYPERPARAMETER TUNING ===
def objective(trial):
    params = {
        'iterations': trial.suggest_int('iterations', 100, 1000),
        'depth': trial.suggest_int('depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
        'border_count': trial.suggest_int('border_count', 32, 255),
        'scale_pos_weight': scale_pos_weight,
        'random_state': 42,
        'verbose': False,
        'thread_count': 1
    }

    model = CatBoostClassifier(**params)
    # Changed to balanced_accuracy for better recall balance
    cv_scores = cross_val_score(model, X_train_balanced, y_train_balanced, cv=cv_splitter, scoring='balanced_accuracy')
    return cv_scores.mean()

# Run optimization with fixed seed
study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
study.optimize(objective, n_trials=200, show_progress_bar=True)

# === TRAIN FINAL MODEL ===
best_params = study.best_params
best_params.update({
    'scale_pos_weight': scale_pos_weight,
    'random_state': 42,
    'verbose': False
})

model = CatBoostClassifier(**best_params)
model.fit(X_train_balanced, y_train_balanced)

# === THRESHOLD OPTIMIZATION ===
print("\nOptimizing prediction threshold...")
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Test different thresholds
thresholds = np.arange(0.1, 0.9, 0.05)
best_threshold = 0.5
best_balanced_acc = 0

threshold_results = []
for threshold in thresholds:
    y_pred_thresh = (y_pred_proba >= threshold).astype(int)
    balanced_acc = balanced_accuracy_score(y_test, y_pred_thresh)
    recall_0 = recall_score(y_test, y_pred_thresh, pos_label=0)
    recall_1 = recall_score(y_test, y_pred_thresh, pos_label=1)

    threshold_results.append({
        'threshold': threshold,
        'balanced_accuracy': balanced_acc,
        'recall_class_0': recall_0,
        'recall_class_1': recall_1
    })

    if balanced_acc > best_balanced_acc:
        best_balanced_acc = balanced_acc
        best_threshold = threshold

print(f"Best threshold: {best_threshold:.2f}")
print(f"Best balanced accuracy: {best_balanced_acc:.4f}")

# === EVALUATION WITH OPTIMIZED THRESHOLD ===
y_pred_default = model.predict(X_test)
y_pred_optimized = (y_pred_proba >= best_threshold).astype(int)

print("\nBest parameters:", best_params)
print(f"\nUsed sampling strategy: {type(best_sampler).__name__}")
print(f"Used scale_pos_weight: {scale_pos_weight:.2f}")

print("\n" + "="*50)
print("MODEL PERFORMANCE - DEFAULT THRESHOLD (0.5)")
print("="*50)
print(f"Accuracy: {accuracy_score(y_test, y_pred_default):.4f}")
print(f"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_default):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_default):.4f}")
print(f"AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")
print(f"Recall Class 0 (No Stroke): {recall_score(y_test, y_pred_default, pos_label=0):.4f}")
print(f"Recall Class 1 (Stroke): {recall_score(y_test, y_pred_default, pos_label=1):.4f}")

print("\nConfusion Matrix (Default):")
cm_default = confusion_matrix(y_test, y_pred_default)
print(cm_default)

print("\n" + "="*50)
print(f"MODEL PERFORMANCE - OPTIMIZED THRESHOLD ({best_threshold:.2f})")
print("="*50)
print(f"Accuracy: {accuracy_score(y_test, y_pred_optimized):.4f}")
print(f"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_optimized):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_optimized):.4f}")
print(f"AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")
print(f"Recall Class 0 (No Stroke): {recall_score(y_test, y_pred_optimized, pos_label=0):.4f}")
print(f"Recall Class 1 (Stroke): {recall_score(y_test, y_pred_optimized, pos_label=1):.4f}")

print("\nConfusion Matrix (Optimized):")
cm_optimized = confusion_matrix(y_test, y_pred_optimized)
print(cm_optimized)

print("\nClassification Report (Optimized Threshold):")
print(classification_report(y_test, y_pred_optimized))

# === FEATURE IMPORTANCE ===
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 10 Most Important Features:")
print(feature_importance.head(10))

# === THRESHOLD ANALYSIS SUMMARY ===
print("\n" + "="*50)
print("THRESHOLD ANALYSIS SUMMARY")
print("="*50)
threshold_df = pd.DataFrame(threshold_results)
print("Top 5 thresholds by balanced accuracy:")
print(threshold_df.nlargest(5, 'balanced_accuracy')[['threshold', 'balanced_accuracy', 'recall_class_0', 'recall_class_1']])

print("\nSampling Strategy Performance:")
for name, score in sampling_results.items():
    print(f"{name}: {score:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import ConfusionMatrixDisplay

# Confusion Matrix (Optimized)
cm = confusion_matrix(y_test, y_pred_optimized)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['No Stroke', 'Stroke'], yticklabels=['No Stroke', 'Stroke'])
plt.title(f'Confusion Matrix (Threshold = {best_threshold:.2f})')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

from sklearn.metrics import roc_curve, auc

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Stroke Classification')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()